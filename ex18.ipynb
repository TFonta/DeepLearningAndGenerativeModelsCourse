{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f30e87",
   "metadata": {},
   "source": [
    "# Vision Transformers (ViT) for Image Classification\n",
    "\n",
    "In this notebook, we'll explore **Vision Transformers**, an architecture that applies the transformer mechanism (originally designed for NLP) to computer vision tasks.\n",
    "\n",
    "## Key Concepts:\n",
    "1. **Patch Embedding**: Images are split into fixed-size patches and linearly embedded\n",
    "2. **Position Embeddings**: Since transformers have no inherent notion of order, we add learnable position embeddings\n",
    "3. **Self-Attention**: The core mechanism that allows patches to \"attend\" to other patches\n",
    "4. **Classification Token**: A special learnable token prepended to the sequence for classification\n",
    "\n",
    "## Why ViT?\n",
    "- **Global context**: Unlike CNNs with local receptive fields, ViT can capture global dependencies from the start\n",
    "- **Scalability**: Transformers scale well with data and model size\n",
    "- **Transfer learning**: Pre-trained ViTs work exceptionally well for downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b5d515",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7958ec8a",
   "metadata": {},
   "source": [
    "## 2. Understanding the ViT Architecture\n",
    "\n",
    "Let's build a Vision Transformer from scratch to understand each component.\n",
    "\n",
    "### 2.1 Patch Embedding\n",
    "\n",
    "The first step is to divide the image into patches and embed them into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d591ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits image into patches and embeds them.\n",
    "    \n",
    "    For a 32x32 image with 4x4 patches:\n",
    "    - We get 8x8 = 64 patches\n",
    "    - Each patch is flattened: 4*4*3 = 48 values\n",
    "    - Then projected to embedding dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2  # Number of patches\n",
    "        \n",
    "        # We can use a Conv2d layer with kernel=patch_size and stride=patch_size\n",
    "        # This is equivalent to splitting into patches and projecting\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, \n",
    "                                   kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, channels, height, width)\n",
    "        x = self.projection(x)  # (batch_size, embed_dim, n_patches**0.5, n_patches**0.5)\n",
    "        x = x.flatten(2)  # (batch_size, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (batch_size, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "# Test the patch embedding\n",
    "patch_embed = PatchEmbedding(img_size=32, patch_size=4, embed_dim=128)\n",
    "dummy_img = torch.randn(2, 3, 32, 32)  # batch of 2 images\n",
    "patches = patch_embed(dummy_img)\n",
    "print(f\"Input shape: {dummy_img.shape}\")\n",
    "print(f\"Patches shape: {patches.shape}\")  # Should be (2, 64, 128)\n",
    "print(f\"Number of patches: {patch_embed.n_patches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e281702",
   "metadata": {},
   "source": [
    "### 2.2 Multi-Head Self-Attention\n",
    "\n",
    "The core of the transformer architecture. It allows patches to attend to all other patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da825c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention mechanism.\n",
    "    \n",
    "    Key idea: Instead of one attention, we have multiple \"heads\" that can \n",
    "    attend to different aspects of the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=128, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, n_tokens, embed_dim = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x)  # (batch_size, n_tokens, 3*embed_dim)\n",
    "        qkv = qkv.reshape(batch_size, n_tokens, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, n_tokens, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        # scores = Q @ K^T / sqrt(head_dim)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention, v)  # (batch_size, num_heads, n_tokens, head_dim)\n",
    "        out = out.transpose(1, 2)  # (batch_size, n_tokens, num_heads, head_dim)\n",
    "        out = out.reshape(batch_size, n_tokens, embed_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "# Test attention\n",
    "attention = MultiHeadAttention(embed_dim=128, num_heads=4)\n",
    "out = attention(patches)\n",
    "print(f\"After attention shape: {out.shape}\")  # Should be (2, 64, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a2414",
   "metadata": {},
   "source": [
    "### 2.3 MLP (Feed-Forward) Block\n",
    "\n",
    "After attention, we apply a feed-forward network to each token independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb82501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (Feed-Forward Network).\n",
    "    Typically: Linear -> GELU -> Dropout -> Linear -> Dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=128, hidden_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)  # GELU activation is commonly used in transformers\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76daf06d",
   "metadata": {},
   "source": [
    "### 2.4 Transformer Block\n",
    "\n",
    "Combines attention and MLP with residual connections and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84dd9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    Structure:\n",
    "        1. LayerNorm -> MultiHeadAttention -> Residual\n",
    "        2. LayerNorm -> MLP -> Residual\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=128, num_heads=4, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, embed_dim * mlp_ratio, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Attention block with residual\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        # MLP block with residual\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f56807",
   "metadata": {},
   "source": [
    "### 2.5 Complete Vision Transformer\n",
    "\n",
    "Now we put everything together into a complete ViT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f254ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Vision Transformer for image classification.\n",
    "    \n",
    "    Args:\n",
    "        img_size: Input image size (assumes square images)\n",
    "        patch_size: Size of each patch\n",
    "        in_channels: Number of input channels (3 for RGB)\n",
    "        num_classes: Number of output classes\n",
    "        embed_dim: Embedding dimension\n",
    "        depth: Number of transformer blocks\n",
    "        num_heads: Number of attention heads\n",
    "        mlp_ratio: Ratio of mlp hidden dim to embedding dim\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
    "                 embed_dim=128, depth=6, num_heads=4, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # Class token - a learnable embedding prepended to the sequence\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Position embeddings - learnable for each position (including cls token)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (batch_size, num_patches, embed_dim)\n",
    "        \n",
    "        # Prepend class token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (batch_size, num_patches+1, embed_dim)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Layer norm and extract class token\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]  # Take the class token\n",
    "        \n",
    "        # Classification\n",
    "        out = self.head(cls_token_final)\n",
    "        return out\n",
    "\n",
    "# Create a small ViT model for CIFAR-10\n",
    "model = VisionTransformer(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    in_channels=3,\n",
    "    num_classes=10,\n",
    "    embed_dim=128,\n",
    "    depth=3,\n",
    "    num_heads=4,\n",
    "    mlp_ratio=4,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(4, 3, 32, 32).to(device)\n",
    "output = model(test_input)\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2363a",
   "metadata": {},
   "source": [
    "## 3. Load CIFAR-10 Dataset\n",
    "\n",
    "We'll use CIFAR-10 because it has small 32x32 images, making training computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                             download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                            download=True, transform=transform_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                         shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                        shuffle=False, num_workers=2)\n",
    "\n",
    "# Class names\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "          'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {len(classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0b7ee",
   "metadata": {},
   "source": [
    "### Visualize some training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid of images\n",
    "imshow(torchvision.utils.make_grid(images[:8]))\n",
    "\n",
    "print('Labels: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb5f20",
   "metadata": {},
   "source": [
    "## 4. Training the Vision Transformer\n",
    "\n",
    "Let's define our training and evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d100fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss / (pbar.n + 1),\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c379d",
   "metadata": {},
   "source": [
    "### Setup training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh model\n",
    "model = VisionTransformer(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    in_channels=3,\n",
    "    num_classes=10,\n",
    "    embed_dim=128, # usually 512 or 768\n",
    "    depth=3, # usually 12\n",
    "    num_heads=4, # usually 8\n",
    "    mlp_ratio=4,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer - AdamW is commonly used for transformers\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "\n",
    "# Learning rate scheduler - cosine annealing\n",
    "num_epochs = 5\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Training for {num_epochs} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df397862",
   "metadata": {},
   "source": [
    "### Run training loop\n",
    "\n",
    "**Note**: Training from scratch can take time. For a quick demo, you can reduce `num_epochs` to 10-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': []\n",
    "}\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "    print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'res/vit_cifar10_best.pth')\n",
    "        print(f\"âœ“ New best model saved! (Acc: {best_acc:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training completed! Best test accuracy: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf3375",
   "metadata": {},
   "source": [
    "### Visualize training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6192c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['test_loss'], label='Test Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Test Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['test_acc'], label='Test Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Test Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Test Accuracy: {history['test_acc'][-1]:.2f}%\")\n",
    "print(f\"Best Test Accuracy: {max(history['test_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e04140",
   "metadata": {},
   "source": [
    "## 5. Testing and Visualization\n",
    "\n",
    "Let's test our model on some examples and visualize the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6052b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('res/vit_cifar10_best.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get a batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = outputs.max(1)\n",
    "    probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(3, 4, figsize=(15, 11))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < 12:\n",
    "        # Display image\n",
    "        img = images[idx].cpu()\n",
    "        img = img / 2 + 0.5  # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        \n",
    "        # Get prediction info\n",
    "        true_label = classes[labels[idx]]\n",
    "        pred_label = classes[predicted[idx]]\n",
    "        confidence = probs[idx][predicted[idx]].item() * 100\n",
    "        \n",
    "        # Set title with color coding\n",
    "        color = 'green' if predicted[idx] == labels[idx] else 'red'\n",
    "        title = f'True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)'\n",
    "        ax.set_title(title, color=color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy for this batch\n",
    "correct = (predicted == labels).sum().item()\n",
    "accuracy = 100. * correct / labels.size(0)\n",
    "print(f\"\\nBatch accuracy: {accuracy:.2f}% ({correct}/{labels.size(0)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222c290",
   "metadata": {},
   "source": [
    "### Per-class accuracy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i].item()\n",
    "            class_total[label] += 1\n",
    "            if predicted[i] == labels[i]:\n",
    "                class_correct[label] += 1\n",
    "\n",
    "# Print and plot results\n",
    "print(\"Per-class accuracy:\\n\" + \"=\"*40)\n",
    "accuracies = []\n",
    "for i in range(10):\n",
    "    acc = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "    accuracies.append(acc)\n",
    "    print(f\"{classes[i]:10s}: {acc:.2f}% ({class_correct[i]}/{class_total[i]})\")\n",
    "\n",
    "# Visualize per-class accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(classes, accuracies, color='steelblue', alpha=0.8)\n",
    "plt.axhline(y=np.mean(accuracies), color='r', linestyle='--', \n",
    "            label=f'Mean: {np.mean(accuracies):.2f}%', linewidth=2)\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Per-Class Accuracy of Vision Transformer on CIFAR-10', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 100)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
