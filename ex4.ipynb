{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb108bd-b0a0-4f07-823d-edee43b9fbef",
   "metadata": {},
   "source": [
    "# Working with Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ff1a8-4842-452f-845d-735efbc97107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd26f7-914f-4ae1-9e9b-b7bb1b3881d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single convolution\n",
    "# kernel_size: the receptive field of the filter \n",
    "# stride: controls the stride for the cross-correlation, a single number or a tuple.\n",
    "# padding: controls the amount of padding applied to the input. It can also\n",
    "# be a string {'valid', 'same'} \n",
    "conv = nn.Conv2d(in_channels=3, \n",
    "                 out_channels=32,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=0\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96d97d-2b65-4420-b979-851d023a8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass some input through the convolutional layer\n",
    "x = torch.randn(1,3,128,128) # has to have 3 channels because of the in_channels value\n",
    "y = conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379ce98-37e9-4934-be4a-749b39b6bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What size the output will be?\n",
    "print(y.size())\n",
    "# WHY?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f199fa75-97ed-4741-81c8-33e2c03425f6",
   "metadata": {},
   "source": [
    "To calculate the output of a convolution you can use this simple equation:\n",
    "\n",
    "$$out\\_size = \\lfloor(W-K+2P)/S\\rfloor+1$$\n",
    "\n",
    "Where:\n",
    "- $W$ = input size \n",
    "- $K$ = kernel size\n",
    "- $P$ = padding\n",
    "- $S$ = stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8eb486-e44b-4e1d-b242-0bd71b38153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore to mantain the same height and width:\n",
    "conv = nn.Conv2d(in_channels=3, \n",
    "                 out_channels=32,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=1 #!!!\n",
    "                )\n",
    "y = conv(x)\n",
    "print(y.size())\n",
    "# or:\n",
    "conv = nn.Conv2d(in_channels=3, \n",
    "                 out_channels=32,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding='same' #!!! (only in conv with stride == 1)\n",
    "                )\n",
    "y = conv(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa207850-70d2-4ffe-b611-8122f9265cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about reducing the dimension by half?\n",
    "# Two option: pooling\n",
    "pool_conv = nn.Sequential(nn.Conv2d(in_channels=3, \n",
    "                         out_channels=32,\n",
    "                         kernel_size=3,\n",
    "                         stride=1,\n",
    "                         padding=1),\n",
    "                         nn.MaxPool2d(kernel_size=2)) # pooling layer: does not have learnable parameters\n",
    "y = pool_conv(x)\n",
    "print(y.size())\n",
    "# or stride\n",
    "down_conv = nn.Conv2d(in_channels=3, \n",
    "                 out_channels=32,\n",
    "                 kernel_size=3,\n",
    "                 stride=2, #!!!\n",
    "                 padding=1 \n",
    "                )\n",
    "y = down_conv(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb03bf5-5321-49b6-a02c-fc0711aed88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about doubling the output size?\n",
    "# Two option: upsampling\n",
    "up_conv = nn.Sequential(nn.Conv2d(in_channels=3, \n",
    "                        out_channels=32,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1),\n",
    "                        nn.Upsample(scale_factor=2)) # upsampling layer: does not have learnable parameters\n",
    "y = up_conv(x)\n",
    "print(y.size())\n",
    "# or transposed convolution\n",
    "tran_conv = nn.ConvTranspose2d(in_channels=3, # learn the upsampling\n",
    "                         out_channels=32,\n",
    "                         kernel_size=4, # !!!\n",
    "                         stride=2,\n",
    "                         padding=1\n",
    "                        )\n",
    "y = tran_conv(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eeab3c-5fc7-4a6b-b87e-f8226e30ab52",
   "metadata": {},
   "source": [
    "To calculate output size of transposed convolution the formula becomes:\n",
    "$$out\\_size = (W - 1)S -2P + (K - 1) + 1$$\n",
    "\n",
    "IMPORTANT: in Generative models it is better to use the manual upsample,\n",
    "because ConvTranspose2d tends to produce checkboard artifacts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643cf99-a632-4323-b071-4dfb3075747b",
   "metadata": {},
   "source": [
    "# Working with image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f66fe-092c-478d-9142-5e1ab7f337bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from PIL import Image # pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc290e-99cf-40a5-bd96-43677f83d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "img = Image.open('res/bruce.png').convert('RGB')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b66d1-3f15-41f9-a459-b19d123d0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to pytorch Tensor and resize it\n",
    "# Use transforms!\n",
    "transform = transforms.Compose([transforms.Resize((256,256)),\n",
    "                                transforms.ToTensor()]) # remove the alpha channel if present\n",
    "img_tensor = transform(img)\n",
    "print(img_tensor.size())\n",
    "print(img_tensor.max(), img_tensor.min()) # ToTensor normalizes the image between [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e7b90-6571-4259-ab4a-0b3e5ac81b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to PIL image\n",
    "to_pil = transforms.ToPILImage()\n",
    "img_pil = to_pil(img_tensor) # back to values between [0,255]\n",
    "print(img_pil.getextrema())\n",
    "plt.imshow(img_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85c8a3-e92a-4b27-8746-3d8f4430a699",
   "metadata": {},
   "source": [
    "### Data augmentation and transformation\n",
    "\n",
    "Often transformations need to be applied to the data in order to be adapted to a particular architecture.\n",
    "In addition, sometimes it is useful to artificially augment the data in order to increase the size and the diversity of images present in the dataset. \n",
    "\n",
    "Common transfomations include:\n",
    "- Resize\n",
    "- Normalize\n",
    "- Random Rotation\n",
    "- Random Crop\n",
    "- Center Crop\n",
    "- Gaussian Blur\n",
    "- Gray Scale\n",
    "\n",
    "Usually <code>p</code> stands for the probability to apply a target transformation to an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334b45e-5998-46f9-ad7b-84825918666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with random horizontal flip\n",
    "rhf = transforms.RandomHorizontalFlip(p=0.5)\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(12):\n",
    "    plt.subplot(10,6,i+1)\n",
    "    plt.imshow(rhf(img_pil))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad170493-1222-4297-aecd-93a513f1204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with Gaussian Blur\n",
    "gb = transforms.GaussianBlur(kernel_size=13)\n",
    "plt.imshow(gb(img_pil))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829bd6b-0bcb-4ac6-8e90-bd8d9bd8f485",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Often the input of a CNN is normalized between [-1,+1]\n",
    "\n",
    "$$\\hat{x}_c = \\frac{x_c - \\mu_c}{\\sigma_c}$$\n",
    "\n",
    "Where $\\hat{x}$ is the normalize data, $c$ is the channel, $\\mu$ is the mean and $\\sigma$ is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b437320f-309d-4a50-84a3-1e970201bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization example\n",
    "norm = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "img_tensor_norm = norm(img_tensor)\n",
    "print(img_tensor_norm.min(), img_tensor_norm.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47401bd8-1725-4976-abfc-d7a6e503e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you try to visualize the normalized image it will look bad\n",
    "plt.imshow(to_pil(img_tensor_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e9ccd-d9ca-4246-a667-702f5de8c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we need to manually denormalize the image\n",
    "def denorm(x):\n",
    "    # Convert the range from [-1, 1] to [0, 1].\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp_(0, 1)\n",
    "\n",
    "plt.imshow(to_pil(denorm(img_tensor_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf776d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some datasets use custom normalization values, like ImageNet:\n",
    "imagenet_norm = transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "img_tensor_imagenet = imagenet_norm(img_tensor)\n",
    "print(img_tensor_imagenet.min(), img_tensor_imagenet.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2890f-3dff-435d-8011-4e8542ce8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex1: load an image, resize it to (256,256), the apply the following transformation:\n",
    "# random crop of (224,224), random horizontal flip with a probability of 85%,\n",
    "# conversion to tensor and normalization between [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b7534-62c3-40ef-8614-49c10f366499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex2: pass the image through two convolutional layer,\n",
    "# the first one has to produce an output of size (1,128,112,112),\n",
    "# the second one has to produce an output of dimension (1,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf109eec-9ce2-46aa-8d05-e64992fa1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex3: visualize the output of the last conv layer: how does it look like?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
