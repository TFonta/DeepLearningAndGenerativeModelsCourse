{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67331446-50d9-4e41-88e4-06fd44063eb9",
   "metadata": {},
   "source": [
    "# Activation Functions in Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039955c-65b4-441e-94d5-a74799e2db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c87b3-ea2b-4eb7-a4d1-6286ea794249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from res.plot_lib import plot_data, plot_data_np, plot_model, set_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd996a0b-faa6-4040-90eb-76d37e9029d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiale default plotting parameters\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f4909-115d-4064-a165-f22febc1cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76febc20-2c43-4f23-9287-425e83a41360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04af42-d145-4d79-afdb-49784a727b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate circles\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "plot_data_np(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5b3f3-b264-4233-b6fe-c85680b3b4b7",
   "metadata": {},
   "source": [
    "### Define a Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4461fd8-70ec-4af6-b3f3-d8d5bd558293",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004fccb9-c566-4b3a-96d4-3f93eeab427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy to tensor\n",
    "X_train, X_test = X[:train_samples, :], X[train_samples:, :]\n",
    "y_train, y_test = y[:train_samples], y[train_samples:]\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2513668c-5713-4fbb-9d6f-81772fbc4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "N = 500  # num epochs\n",
    "D = 2  # input dimensions\n",
    "C = 2  # num classes\n",
    "H = 10  # num_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2a9d2-ed68-44ae-8167-893922111214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package to create our linear model\n",
    "# each Linear module has a weight and bias\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "model.to(device) #Convert to CUDA\n",
    "\n",
    "# nn package also has different loss functions.\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# we use the optim package to apply\n",
    "# stochastic gradient descent for our parameter updates\n",
    "#optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21942f0-2d59-48b7-a9fb-b6fa1fb72ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_model(model, X_train, y_train, X_test, y_test, criterion):\n",
    "    loss_values_train = []\n",
    "    acc_values_train = []\n",
    "    acc_values_test = []\n",
    "    for t in range(N):\n",
    "        # shuffle training data\n",
    "        perm = torch.randperm(X_train.size(0))\n",
    "        X_train = X_train[perm]\n",
    "        y_train = y_train[perm]\n",
    "        epoch_acc = 0\n",
    "        epoch_loss = 0\n",
    "        # Mini-batch training\n",
    "        for batch in range(0, len(X_train), 100):\n",
    "            X_batch = X_train[batch:batch+100]\n",
    "            y_batch = y_train[batch:batch+100]\n",
    "            # Feed forward to get the logits\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            # Compute the loss and accuracy\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            score, predicted = torch.max(y_pred, 1)\n",
    "            acc = (y_batch == predicted).sum().float() / len(y_batch)\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            # zero the gradients before running\n",
    "            # the backward pass.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass to compute the gradient\n",
    "            # of loss w.r.t our learnable params. \n",
    "            loss.backward()\n",
    "\n",
    "            # Update params\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Average loss and acc over epoch\n",
    "        if (t+1) % 10 == 0:            \n",
    "            loss = epoch_loss / (len(X_train) / 100)\n",
    "            acc = epoch_acc / (len(X_train) / 100)\n",
    "            # Test model\n",
    "            acc_test = test_model(model, X_test, y_test)\n",
    "            \n",
    "            print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY TRAIN]: %.3f, \\\n",
    "                [ACCURACY TEST]: %.3f\" % (t, loss, acc, acc_test))\n",
    "            #display.clear_output(wait=True)\n",
    "            \n",
    "            # Save loss and acc values\n",
    "            loss_values_train.append(loss)\n",
    "            acc_values_train.append(acc)\n",
    "            acc_values_test.append(acc_test.item())\n",
    "    return loss_values_train, acc_values_train, acc_values_test\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(model, X, y):\n",
    "    # Feed forward to get the logits\n",
    "    y_pred = model(X)\n",
    "    # Get accuracy\n",
    "    score, predicted = torch.max(y_pred, 1)\n",
    "    acc = (y == predicted).sum().float() / len(y) \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8152af21-a93e-4b36-a79b-bbe7a671f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_val_train, acc_val_train, acc_val_test = train_model(model = model, \n",
    "                                                       X_train = X_train, y_train = y_train, \n",
    "                                                       X_test = X_test, y_test = y_test,\n",
    "                                                       criterion = criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78a0da-0073-4be1-bfcc-a794d115dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e92057-3729-4093-aed7-9001045593c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss and Acc\n",
    "fig, (ax1, ax2) = pyplot.subplots(1, 2)\n",
    "fig.set_size_inches(20, 10)\n",
    "fig.suptitle('Loss and Acc')\n",
    "ax1.plot(l_val_train, label='train_loss', color = 'red')\n",
    "ax1.legend()\n",
    "ax2.plot(acc_val_train, label='train_acc', color = 'blue')\n",
    "ax2.plot(acc_val_test, label='test_acc', color = 'green')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556b53b-2d01-4abf-99c7-b0a925dbdfdc",
   "metadata": {},
   "source": [
    "### Add activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7531b4-5816-4491-8d59-a51a270fbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# nn package also has different loss functions.\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# we use the optim package to apply\n",
    "# SGD for our parameter updates\n",
    "#optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0.9)\n",
    "\n",
    "# Training\n",
    "l_val_train, acc_val_train, acc_val_test = train_model(model = model, \n",
    "                                                       X_train = X_train, y_train = y_train, \n",
    "                                                       X_test = X_test, y_test = y_test,\n",
    "                                                       criterion = criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe001f5-7d69-4ffb-8175-3f8a7e5df550",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d85720-8498-4739-ac76-8e35d847e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss and Acc\n",
    "fig, (ax1, ax2) = pyplot.subplots(1, 2)\n",
    "fig.set_size_inches(20, 10)\n",
    "fig.suptitle('Loss and Acc')\n",
    "ax1.plot(l_val_train, label='train_loss', color = 'red')\n",
    "ax1.legend()\n",
    "ax2.plot(acc_val_train, label='train_acc', color = 'blue')\n",
    "ax2.plot(acc_val_test, label='test_acc', color = 'green')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea5902-e916-49d3-a0ec-7b36692d2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a deeper model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(H, H),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# nn package also has different loss functions.\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# we use the optim package to apply\n",
    "# SGD for our parameter updates\n",
    "#optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0.9)\n",
    "\n",
    "# Training\n",
    "l_val_train, acc_val_train, acc_val_test = train_model(model = model, \n",
    "                                                       X_train = X_train, y_train = y_train, \n",
    "                                                       X_test = X_test, y_test = y_test,\n",
    "                                                       criterion = criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f582bf-3342-4701-b0ef-6c738409b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab6010-f684-418f-a56e-7bf19fcc9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss and Acc\n",
    "fig, (ax1, ax2) = pyplot.subplots(1, 2)\n",
    "fig.set_size_inches(20, 10)\n",
    "fig.suptitle('Loss and Acc')\n",
    "ax1.plot(l_val_train, label='train_loss', color = 'red')\n",
    "ax1.legend()\n",
    "ax2.plot(acc_val_train, label='train_acc', color = 'blue')\n",
    "ax2.plot(acc_val_test, label='test_acc', color = 'green')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de387e-ed50-4664-a9dd-c2e175580976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try an even deeper model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(H, H),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(H, H),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(H, H),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(H, H),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(H, H),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(H, H),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(H, H),\n",
    "    nn.Linear(H, H),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# nn package also has different loss functions.\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# we use the optim package to apply\n",
    "# SGD for our parameter updates\n",
    "#optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0.9)\n",
    "\n",
    "# Training\n",
    "l_val_train, acc_val_train, acc_val_test = train_model(model = model, \n",
    "                                                       X_train = X_train, y_train = y_train, \n",
    "                                                       X_test = X_test, y_test = y_test,\n",
    "                                                       criterion = criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b5e1fc-e7bc-435f-a7e0-01c58f974b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf373d9-a585-4710-8c11-9a5d397a4d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss and Acc\n",
    "fig, (ax1, ax2) = pyplot.subplots(1, 2)\n",
    "fig.set_size_inches(20, 10)\n",
    "fig.suptitle('Loss and Acc')\n",
    "ax1.plot(l_val_train, label='train_loss', color = 'red')\n",
    "ax1.legend()\n",
    "ax2.plot(acc_val_train, label='train_acc', color = 'blue')\n",
    "ax2.plot(acc_val_test, label='test_acc', color = 'green')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16ebd6-4974-4567-9b91-51a41fcb1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try RelU instead of Sigmoid\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# nn package also has different loss functions.\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# we use the optim package to apply\n",
    "# SGD for our parameter updates\n",
    "#optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0.9)\n",
    "\n",
    "# Training\n",
    "l_val_train, acc_val_train, acc_val_test = train_model(model = model, \n",
    "                                                       X_train = X_train, y_train = y_train, \n",
    "                                                       X_test = X_test, y_test = y_test,\n",
    "                                                       criterion = criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a1017-3762-4ec0-91c0-6e41db81f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc29a6-ac57-43c4-88ea-904944b1cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss and Acc\n",
    "fig, (ax1, ax2) = pyplot.subplots(1, 2)\n",
    "fig.set_size_inches(20, 10)\n",
    "fig.suptitle('Loss and Acc')\n",
    "ax1.plot(l_val_train, label='train_loss', color = 'red')\n",
    "ax1.legend()\n",
    "ax2.plot(acc_val_train, label='train_acc', color = 'blue')\n",
    "ax2.plot(acc_val_test, label='test_acc', color = 'green')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4360353-6b87-4235-bd60-fa47fefd31f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex1: Experiment with different activation functions (like leakyrelu or sigmoid) and layers dimension. How the results change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
