{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b0ae32-d608-4464-b5a2-93f3222be82b",
   "metadata": {},
   "source": [
    "# Defining and Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77933bed-341f-4546-a82e-c4f23bd605f9",
   "metadata": {},
   "source": [
    "What we will implement:\n",
    "- How to initialize a NN\n",
    "- Forward pass\n",
    "- Backward pass\n",
    "- Optimization of the network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243891b-5b8c-4d77-9b01-251e9c07c6ff",
   "metadata": {},
   "source": [
    "## Pytorch: <code>nn</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae42ba-4062-4422-b84f-f43d7ab79537",
   "metadata": {},
   "source": [
    "The <code>nn</code> package defines a set of Modules (i.e. neural networks layers).\n",
    "\n",
    "Each module receive an input and produces an output.\n",
    "\n",
    "The <code>nn</code> package also defines losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d20409-f40d-4c15-be65-d1e445b8b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928f2e25-8f2b-425e-84dd-ce1890bd6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "859097be-808f-4736-bb34-4c350d3d206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 3])\n"
     ]
    }
   ],
   "source": [
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "print(xx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd4b337-b68f-4829-b3f9-c8221f71a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5526daa5-aedd-42d0-82dd-09af2fc842be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-6\n",
    "# Construct the Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters (defined \n",
    "# with torch.nn.Parameter) which are members of the model.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a79672-c22e-47c9-9df3-74d8aaf09299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 978.2300415039062\n",
      "199 649.8440551757812\n",
      "299 432.6981506347656\n",
      "399 289.11004638671875\n",
      "499 194.16184997558594\n",
      "599 131.37692260742188\n",
      "699 89.86022186279297\n",
      "799 62.40728759765625\n",
      "899 44.253944396972656\n",
      "999 32.24992752075195\n",
      "1099 24.312265396118164\n",
      "1199 19.06338882446289\n",
      "1299 15.592564582824707\n",
      "1399 13.2974271774292\n",
      "1499 11.779789924621582\n",
      "1599 10.776226043701172\n",
      "1699 10.112586975097656\n",
      "1799 9.673775672912598\n",
      "1899 9.3836030960083\n",
      "1999 9.191720008850098\n"
     ]
    }
   ],
   "source": [
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward() # (y_pred - y).pow(2).sum()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Alternative: Update the weights using gradient descent MANUALLY. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5133d785-f599-4cbe-8933-c718e60ac656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: y = 0.0002835117047652602 + 0.8379294872283936 x + -4.891062417300418e-05 x^2 + -0.0906546413898468 x^3\n"
     ]
    }
   ],
   "source": [
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "090ad36c-c238-4d24-bdfe-6e7fe4f0a426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5700,  2.4649, -3.8700])\n",
      "-0.964564 -1.000000\n"
     ]
    }
   ],
   "source": [
    "# The network has effectively learned something?\n",
    "print(xx[500]) # x[500] = -pi/2\n",
    "print(\"%.6f %.6f\" % (model(xx)[500].item(), torch.sin(x)[500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b9b67-7ecb-4d4e-adbe-9dca48d9bcca",
   "metadata": {},
   "source": [
    "## Custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b499600-b3e9-457f-ba9a-c933c96a0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate all the layer of the NN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Flatten(0, 1)\n",
    "        )\n",
    "        \"\"\"\n",
    "        In alternative we could also define each layer individually\n",
    "        \"\"\"\n",
    "        # self.l1 = nn.Linear(3, 1)\n",
    "        # self.flt = nn.Flatten(0, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "        # x = self.l1(x)\n",
    "        # return self.flt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cc6349f-ce73-4c63-8309-fa600a765b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above\n",
    "model = SinModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dd2d81e-c1a3-4c46-83d4-c31ce20d6f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 9.038326263427734\n",
      "199 8.972274780273438\n",
      "299 8.926005363464355\n",
      "399 8.893573760986328\n",
      "499 8.870832443237305\n",
      "599 8.854877471923828\n",
      "699 8.843677520751953\n",
      "799 8.835809707641602\n",
      "899 8.830282211303711\n",
      "999 8.826396942138672\n",
      "1099 8.823665618896484\n",
      "1199 8.821743965148926\n",
      "1299 8.820390701293945\n",
      "1399 8.819438934326172\n",
      "1499 8.818768501281738\n",
      "1599 8.818296432495117\n",
      "1699 8.817963600158691\n",
      "1799 8.817728042602539\n",
      "1899 8.8175630569458\n",
      "1999 8.8174467086792\n"
     ]
    }
   ],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters (defined \n",
    "# with torch.nn.Parameter) which are members of the model.\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95f34ff4-0a0e-403d-89ed-9e8f7b30e80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5700,  2.4649, -3.8700])\n",
      "-0.983484 -1.000000\n"
     ]
    }
   ],
   "source": [
    "# The network has effectively learned something?\n",
    "print(xx[500]) # x[500] = -pi/2\n",
    "print(\"%.6f %.6f\" % (model(xx)[500].item(), torch.sin(x)[500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b4585-23a6-4204-b98c-140aa41d0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex1: write a model (using custom modules) where the output y is a linear function of (x, x^2, x^3, x^4)\n",
    "# and it approximate the cosine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611592dd-58ac-40d7-9a4b-0b92fe51bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex2: write a model (using custom modules) where the output y is a linear function of (x, x^2, x^3)\n",
    "# and it approximate the function -5 + 2*x + 3/4x^2 + 7*x^3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
